{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "Y0tbk1zpYlT8",
        "outputId": "723aa544-a76c-4603-ae45-fdb76419f544"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-28479c72-52f6-41fb-be6c-f54257415349\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-28479c72-52f6-41fb-be6c-f54257415349\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving images.zip to images.zip\n",
            "Saving labels.zip to labels (1).zip\n",
            "Skipping '72 (12).jpg' (no label found).\n",
            "Skipping '00364_120.jpg' (no label found).\n",
            "Skipping '00364_113.jpg' (no label found).\n",
            "Skipping '00364_107.jpg' (no label found).\n",
            "Skipping '00364_119.jpg' (no label found).\n",
            "Skipping '00360_108.jpg' (no label found).\n",
            "Skipping '00364_125.jpg' (no label found).\n",
            "Skipping '00360_129.jpg' (no label found).\n",
            "Skipping '00364_127.jpg' (no label found).\n",
            "Skipping '00360_120.jpg' (no label found).\n",
            "Skipping '00406_98.jpg' (no label found).\n",
            "Skipping '00360_126.jpg' (no label found).\n",
            "Skipping '00360_122.jpg' (no label found).\n",
            "Skipping '00360_114.jpg' (no label found).\n",
            "Skipping '00360_115.jpg' (no label found).\n",
            "\n",
            " Saved processed metadata to 'tumor_data.csv'\n",
            " - 1 = tumor present, 0 = no tumor\n",
            " - Only bounding boxes of class 1 are retained.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "\n",
        "# Step 1: Upload image.zip and labels.zip\n",
        "uploaded = files.upload()  # Expecting: images.zip and labels.zip\n",
        "\n",
        "# Step 2: Define target directories\n",
        "base_dir = Path(\"dataset\")\n",
        "images_dir = base_dir / \"images\"\n",
        "labels_dir = base_dir / \"labels\"\n",
        "\n",
        "# Step 3: Create directories\n",
        "images_dir.mkdir(parents=True, exist_ok=True)\n",
        "labels_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Step 4: Unzip and flatten\n",
        "def extract_zip(zip_file, target_dir):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(target_dir)\n",
        "\n",
        "    # Move files from subfolders to the root of target_dir\n",
        "    for root, _, files in os.walk(target_dir):\n",
        "        for f in files:\n",
        "            src = os.path.join(root, f)\n",
        "            dst = os.path.join(target_dir, f)\n",
        "            if src != dst:\n",
        "                os.rename(src, dst)\n",
        "\n",
        "    # Cleanup subfolders\n",
        "    for item in os.listdir(target_dir):\n",
        "        item_path = os.path.join(target_dir, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            os.rmdir(item_path)\n",
        "\n",
        "extract_zip(\"images.zip\", images_dir)\n",
        "extract_zip(\"labels.zip\", labels_dir)\n",
        "\n",
        "# Step 5: Process files\n",
        "image_data = []\n",
        "\n",
        "for img_file in os.listdir(images_dir):\n",
        "    if not img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")):\n",
        "        continue\n",
        "\n",
        "    image_name = Path(img_file).stem\n",
        "    label_file = labels_dir / f\"{image_name}.txt\"\n",
        "    image_path = images_dir / img_file\n",
        "\n",
        "    if not label_file.exists():\n",
        "        print(f\"Skipping '{img_file}' (no label found).\")\n",
        "        os.remove(image_path)\n",
        "        continue\n",
        "\n",
        "    # Read label file\n",
        "    with open(label_file, \"r\") as file:\n",
        "        lines = [l.strip() for l in file.readlines() if l.strip()]\n",
        "\n",
        "    tumor_flag = 0\n",
        "    box_list = []\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.split()\n",
        "        if len(parts) != 5:\n",
        "            continue\n",
        "        class_id, *coords = parts\n",
        "        if class_id == '1':\n",
        "            try:\n",
        "                box = [float(x) for x in coords]\n",
        "                box_list.append(box)\n",
        "                tumor_flag = 1\n",
        "            except ValueError:\n",
        "                print(f\" Invalid format in {label_file.name}: {line}\")\n",
        "                continue\n",
        "\n",
        "    image_data.append({\n",
        "        \"filename\": img_file,\n",
        "        \"has_tumor\": tumor_flag,\n",
        "        \"boxes\": box_list\n",
        "    })\n",
        "\n",
        "# Step 6: Save DataFrame\n",
        "df = pd.DataFrame(image_data, columns=[\"filename\", \"has_tumor\", \"boxes\"])\n",
        "df.to_csv(\"tumor_data.csv\", index=False)\n",
        "print(\"\\n Saved processed metadata to 'tumor_data.csv'\")\n",
        "print(\" - 1 = tumor present, 0 = no tumor\")\n",
        "print(\" - Only bounding boxes of class 1 are retained.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# ========== Step 1: Unzip Image and Label Files ==========\n",
        "import shutil\n",
        "\n",
        "def unzip_and_flatten(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    for root, _, files in os.walk(extract_to):\n",
        "        for file in files:\n",
        "            src = os.path.join(root, file)\n",
        "            dst = os.path.join(extract_to, file)\n",
        "            if src != dst:\n",
        "                shutil.move(src, dst)\n",
        "    for subfolder in os.listdir(extract_to):\n",
        "        subfolder_path = os.path.join(extract_to, subfolder)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            shutil.rmtree(subfolder_path)\n",
        "\n",
        "unzip_and_flatten(\"images.zip\", \"images_data\")\n",
        "unzip_and_flatten(\"labels.zip\", \"labels_data\")\n",
        "\n",
        "# ========== Step 2: Process Data ==========\n",
        "IMG_DIM = 512\n",
        "image_dir = \"images_data\"\n",
        "label_dir = \"labels_data\"\n",
        "\n",
        "inputs, labels = [], []\n",
        "\n",
        "for fname in os.listdir(label_dir):\n",
        "    label_path = os.path.join(label_dir, fname)\n",
        "    image_path = os.path.join(image_dir, fname.replace('.txt', '.jpg'))\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        continue\n",
        "\n",
        "    with open(label_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    has_tumor = any(line.startswith(\"1\") for line in lines if len(line.split()) == 5)\n",
        "    labels.append(1 if has_tumor else 0)\n",
        "\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    # Resize with padding\n",
        "    h, w = img.shape\n",
        "    scale = IMG_DIM / max(h, w)\n",
        "    new_h, new_w = int(h * scale), int(w * scale)\n",
        "    resized = cv2.resize(img, (new_w, new_h))\n",
        "    pad_top = (IMG_DIM - new_h) // 2\n",
        "    pad_bottom = IMG_DIM - new_h - pad_top\n",
        "    pad_left = (IMG_DIM - new_w) // 2\n",
        "    pad_right = IMG_DIM - new_w - pad_left\n",
        "    padded_img = cv2.copyMakeBorder(resized, pad_top, pad_bottom, pad_left, pad_right,\n",
        "                                    cv2.BORDER_CONSTANT, value=0)\n",
        "\n",
        "    # Convert to 3-channel and preprocess\n",
        "    padded_img = np.expand_dims(padded_img, axis=-1)\n",
        "    padded_img = np.repeat(padded_img, 3, axis=-1)\n",
        "    padded_img = preprocess_input(padded_img.astype(np.float32))\n",
        "    inputs.append(padded_img)\n",
        "\n",
        "X = np.array(inputs)\n",
        "y = np.array(labels)\n",
        "\n",
        "# ========== Step 3: Train/Val Split ==========\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ========== Step 4: Data Augmentation ==========\n",
        "augment = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1),\n",
        "])\n",
        "\n",
        "# ========== Step 5: Class Weights ==========\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "weights = dict(enumerate(weights))\n",
        "\n",
        "# ========== Step 6: Build Model ==========\n",
        "base = EfficientNetB0(include_top=False, input_shape=(IMG_DIM, IMG_DIM, 3), weights=\"imagenet\")\n",
        "base.trainable = True\n",
        "for layer in base.layers[:-30]:  # freeze most layers\n",
        "    layer.trainable = False\n",
        "\n",
        "inp = layers.Input(shape=(IMG_DIM, IMG_DIM, 3))\n",
        "x = augment(inp)\n",
        "x = base(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "out = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = models.Model(inputs=inp, outputs=out)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# ========== Step 7: Train ==========\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=4, monitor='val_loss', restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
        "    ModelCheckpoint(\"optimized_classifier.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    class_weight=weights,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# ========== Step 8: Evaluation ==========\n",
        "y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
        "print(\"\\n Classification Report:\")\n",
        "print(classification_report(y_val, y_pred, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvQlVp6kba1r",
        "outputId": "fd94bff7-d565-443b-ad6e-dcec82355f34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.5284 - loss: 0.6865 - precision_1: 0.5574 - recall_1: 0.4363\n",
            "Epoch 1: val_loss improved from inf to 0.66233, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 350ms/step - accuracy: 0.5303 - loss: 0.6860 - precision_1: 0.5587 - recall_1: 0.4441 - val_accuracy: 0.5909 - val_loss: 0.6623 - val_precision_1: 0.5746 - val_recall_1: 0.8370 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6658 - loss: 0.6247 - precision_1: 0.6894 - recall_1: 0.6972\n",
            "Epoch 2: val_loss improved from 0.66233 to 0.64047, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - accuracy: 0.6660 - loss: 0.6247 - precision_1: 0.6896 - recall_1: 0.6955 - val_accuracy: 0.6136 - val_loss: 0.6405 - val_precision_1: 0.6017 - val_recall_1: 0.7717 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.6512 - loss: 0.6170 - precision_1: 0.6930 - recall_1: 0.6087\n",
            "Epoch 3: val_loss improved from 0.64047 to 0.61917, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.6517 - loss: 0.6169 - precision_1: 0.6937 - recall_1: 0.6083 - val_accuracy: 0.6307 - val_loss: 0.6192 - val_precision_1: 0.6392 - val_recall_1: 0.6739 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6573 - loss: 0.5956 - precision_1: 0.7062 - recall_1: 0.6866\n",
            "Epoch 4: val_loss improved from 0.61917 to 0.60066, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - accuracy: 0.6583 - loss: 0.5953 - precision_1: 0.7052 - recall_1: 0.6885 - val_accuracy: 0.6818 - val_loss: 0.6007 - val_precision_1: 0.7368 - val_recall_1: 0.6087 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.7155 - loss: 0.5658 - precision_1: 0.7998 - recall_1: 0.6021\n",
            "Epoch 5: val_loss improved from 0.60066 to 0.58894, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.7154 - loss: 0.5655 - precision_1: 0.7982 - recall_1: 0.6045 - val_accuracy: 0.6705 - val_loss: 0.5889 - val_precision_1: 0.6667 - val_recall_1: 0.7391 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.7248 - loss: 0.5227 - precision_1: 0.7477 - recall_1: 0.7164\n",
            "Epoch 6: val_loss improved from 0.58894 to 0.56438, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - accuracy: 0.7255 - loss: 0.5227 - precision_1: 0.7485 - recall_1: 0.7169 - val_accuracy: 0.6761 - val_loss: 0.5644 - val_precision_1: 0.6733 - val_recall_1: 0.7391 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.7697 - loss: 0.5032 - precision_1: 0.7864 - recall_1: 0.7823\n",
            "Epoch 7: val_loss improved from 0.56438 to 0.53806, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.7686 - loss: 0.5036 - precision_1: 0.7851 - recall_1: 0.7812 - val_accuracy: 0.7784 - val_loss: 0.5381 - val_precision_1: 0.8272 - val_recall_1: 0.7283 - learning_rate: 1.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.7991 - loss: 0.4641 - precision_1: 0.8576 - recall_1: 0.7691\n",
            "Epoch 8: val_loss improved from 0.53806 to 0.52151, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.7985 - loss: 0.4644 - precision_1: 0.8551 - recall_1: 0.7698 - val_accuracy: 0.7273 - val_loss: 0.5215 - val_precision_1: 0.7973 - val_recall_1: 0.6413 - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.7928 - loss: 0.4620 - precision_1: 0.8539 - recall_1: 0.7260\n",
            "Epoch 9: val_loss did not improve from 0.52151\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.7935 - loss: 0.4616 - precision_1: 0.8531 - recall_1: 0.7288 - val_accuracy: 0.7500 - val_loss: 0.5221 - val_precision_1: 0.7222 - val_recall_1: 0.8478 - learning_rate: 1.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8328 - loss: 0.4461 - precision_1: 0.8141 - recall_1: 0.8772\n",
            "Epoch 10: val_loss improved from 0.52151 to 0.48671, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - accuracy: 0.8332 - loss: 0.4446 - precision_1: 0.8156 - recall_1: 0.8758 - val_accuracy: 0.7955 - val_loss: 0.4867 - val_precision_1: 0.8111 - val_recall_1: 0.7935 - learning_rate: 1.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8278 - loss: 0.4053 - precision_1: 0.8155 - recall_1: 0.8723\n",
            "Epoch 11: val_loss improved from 0.48671 to 0.48636, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - accuracy: 0.8284 - loss: 0.4046 - precision_1: 0.8167 - recall_1: 0.8715 - val_accuracy: 0.7670 - val_loss: 0.4864 - val_precision_1: 0.8312 - val_recall_1: 0.6957 - learning_rate: 1.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.7938 - loss: 0.4440 - precision_1: 0.8264 - recall_1: 0.7815\n",
            "Epoch 12: val_loss did not improve from 0.48636\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.7948 - loss: 0.4428 - precision_1: 0.8263 - recall_1: 0.7835 - val_accuracy: 0.7614 - val_loss: 0.5054 - val_precision_1: 0.8788 - val_recall_1: 0.6304 - learning_rate: 1.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8165 - loss: 0.3895 - precision_1: 0.9082 - recall_1: 0.7276\n",
            "Epoch 13: val_loss improved from 0.48636 to 0.45181, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - accuracy: 0.8174 - loss: 0.3887 - precision_1: 0.9059 - recall_1: 0.7318 - val_accuracy: 0.8125 - val_loss: 0.4518 - val_precision_1: 0.8471 - val_recall_1: 0.7826 - learning_rate: 1.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8565 - loss: 0.3439 - precision_1: 0.8943 - recall_1: 0.8261\n",
            "Epoch 14: val_loss improved from 0.45181 to 0.43681, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.8567 - loss: 0.3437 - precision_1: 0.8936 - recall_1: 0.8272 - val_accuracy: 0.8125 - val_loss: 0.4368 - val_precision_1: 0.8041 - val_recall_1: 0.8478 - learning_rate: 1.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8871 - loss: 0.3335 - precision_1: 0.9055 - recall_1: 0.8726\n",
            "Epoch 15: val_loss did not improve from 0.43681\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.8870 - loss: 0.3331 - precision_1: 0.9048 - recall_1: 0.8733 - val_accuracy: 0.7898 - val_loss: 0.4597 - val_precision_1: 0.7670 - val_recall_1: 0.8587 - learning_rate: 1.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8820 - loss: 0.2960 - precision_1: 0.9065 - recall_1: 0.8691\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\n",
            "Epoch 16: val_loss did not improve from 0.43681\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.8816 - loss: 0.2963 - precision_1: 0.9054 - recall_1: 0.8695 - val_accuracy: 0.8011 - val_loss: 0.4572 - val_precision_1: 0.8608 - val_recall_1: 0.7391 - learning_rate: 1.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8982 - loss: 0.2695 - precision_1: 0.9153 - recall_1: 0.8794\n",
            "Epoch 17: val_loss did not improve from 0.43681\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.8973 - loss: 0.2701 - precision_1: 0.9142 - recall_1: 0.8791 - val_accuracy: 0.7386 - val_loss: 0.5178 - val_precision_1: 0.6917 - val_recall_1: 0.9022 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9228 - loss: 0.2417 - precision_1: 0.9258 - recall_1: 0.9268\n",
            "Epoch 18: val_loss improved from 0.43681 to 0.42680, saving model to optimized_classifier.keras\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.9220 - loss: 0.2424 - precision_1: 0.9254 - recall_1: 0.9255 - val_accuracy: 0.8125 - val_loss: 0.4268 - val_precision_1: 0.8172 - val_recall_1: 0.8261 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9099 - loss: 0.2492 - precision_1: 0.9087 - recall_1: 0.9239\n",
            "Epoch 19: val_loss did not improve from 0.42680\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.9090 - loss: 0.2501 - precision_1: 0.9074 - recall_1: 0.9236 - val_accuracy: 0.8182 - val_loss: 0.4356 - val_precision_1: 0.8409 - val_recall_1: 0.8043 - learning_rate: 5.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9150 - loss: 0.2372 - precision_1: 0.9575 - recall_1: 0.8783\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\n",
            "Epoch 20: val_loss did not improve from 0.42680\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.9148 - loss: 0.2382 - precision_1: 0.9568 - recall_1: 0.8785 - val_accuracy: 0.7670 - val_loss: 0.4863 - val_precision_1: 0.7297 - val_recall_1: 0.8804 - learning_rate: 5.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8901 - loss: 0.2475 - precision_1: 0.8781 - recall_1: 0.9227\n",
            "Epoch 21: val_loss did not improve from 0.42680\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.8900 - loss: 0.2475 - precision_1: 0.8784 - recall_1: 0.9219 - val_accuracy: 0.8011 - val_loss: 0.4323 - val_precision_1: 0.8065 - val_recall_1: 0.8152 - learning_rate: 2.5000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9030 - loss: 0.2352 - precision_1: 0.9223 - recall_1: 0.8879\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "\n",
            "Epoch 22: val_loss did not improve from 0.42680\n",
            "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.9032 - loss: 0.2353 - precision_1: 0.9224 - recall_1: 0.8881 - val_accuracy: 0.7841 - val_loss: 0.4476 - val_precision_1: 0.7596 - val_recall_1: 0.8587 - learning_rate: 2.5000e-05\n",
            "Epoch 22: early stopping\n",
            "Restoring model weights from the end of the best epoch: 18.\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 534ms/step\n",
            "\n",
            " Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8072    0.7976    0.8024        84\n",
            "           1     0.8172    0.8261    0.8216        92\n",
            "\n",
            "    accuracy                         0.8125       176\n",
            "   macro avg     0.8122    0.8119    0.8120       176\n",
            "weighted avg     0.8124    0.8125    0.8124       176\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Required Libraries\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Unzip Data\n",
        "def unzip_and_flatten(zip_path, target_dir):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(target_dir)\n",
        "\n",
        "    # Flatten nested dirs if any\n",
        "    for root, _, files in os.walk(target_dir):\n",
        "        for file in files:\n",
        "            src = os.path.join(root, file)\n",
        "            dst = os.path.join(target_dir, file)\n",
        "            if src != dst:\n",
        "                shutil.move(src, dst)\n",
        "    # Cleanup folders\n",
        "    for f in os.listdir(target_dir):\n",
        "        p = os.path.join(target_dir, f)\n",
        "        if os.path.isdir(p): shutil.rmtree(p)\n",
        "\n",
        "# Input ZIP files\n",
        "image_zip = \"images.zip\"\n",
        "label_zip = \"labels.zip\"\n",
        "\n",
        "# Extraction directories\n",
        "raw_dir = Path(\"raw_data\")\n",
        "img_dir = raw_dir / \"images\"\n",
        "lbl_dir = raw_dir / \"labels\"\n",
        "shutil.rmtree(raw_dir, ignore_errors=True)\n",
        "img_dir.mkdir(parents=True)\n",
        "lbl_dir.mkdir(parents=True)\n",
        "\n",
        "# Extract\n",
        "unzip_and_flatten(image_zip, img_dir)\n",
        "unzip_and_flatten(label_zip, lbl_dir)\n",
        "\n",
        "# Split and Filter\n",
        "image_list = sorted([f for f in img_dir.glob(\"*.jpg\")])\n",
        "train_files, val_files = train_test_split(image_list, test_size=0.2, random_state=42)\n",
        "\n",
        "# YOLO-style structure\n",
        "base = Path(\"yolo_dataset\")\n",
        "for folder in ['images/train', 'images/val', 'labels/train', 'labels/val']:\n",
        "    (base / folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def prepare_split(images, split):\n",
        "    for img_path in images:\n",
        "        name = img_path.stem\n",
        "        label_path = lbl_dir / f\"{name}.txt\"\n",
        "        if not label_path.exists():\n",
        "            continue\n",
        "\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        tumor_labels = []\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            if parts and parts[0] == \"1\":\n",
        "                parts[0] = \"0\"\n",
        "                tumor_labels.append(\" \".join(parts))\n",
        "\n",
        "        if tumor_labels:\n",
        "            shutil.copy(img_path, base / f\"images/{split}\" / img_path.name)\n",
        "            with open(base / f\"labels/{split}\" / f\"{name}.txt\", \"w\") as f:\n",
        "                f.write(\"\\n\".join(tumor_labels))\n",
        "\n",
        "prepare_split(train_files, \"train\")\n",
        "prepare_split(val_files, \"val\")\n",
        "\n",
        "# Generate YAML\n",
        "dataset_yaml = f\"\"\"\n",
        "path: {base.resolve()}\n",
        "train: images/train\n",
        "val: images/val\n",
        "names:\n",
        "  0: tumor\n",
        "\"\"\"\n",
        "yaml_path = base / \"tumor_data.yaml\"\n",
        "with open(yaml_path, \"w\") as f:\n",
        "    f.write(dataset_yaml.strip())\n",
        "\n",
        "# Train YOLOv8\n",
        "!pip install -q ultralytics\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "\n",
        "model = YOLO(\"yolov8s.pt\")  # we can also use yolov8m.pt for better performance\n",
        "\n",
        "model.train(\n",
        "    data=str(yaml_path),\n",
        "    epochs=150,\n",
        "    imgsz=512,\n",
        "    batch=16,\n",
        "    lr0=0.002,\n",
        "    optimizer='AdamW',\n",
        "    weight_decay=0.0005,\n",
        "    dropout=0.2,\n",
        "    patience=20,\n",
        "    mixup=0.1,\n",
        "    cos_lr=True,\n",
        "    auto_augment='randaugment',\n",
        "    hsv_h=0.015,\n",
        "    hsv_s=0.7,\n",
        "    hsv_v=0.4,\n",
        "    translate=0.1,\n",
        "    scale=0.4,\n",
        "    shear=2.0,\n",
        "    perspective=0.0005,\n",
        "    flipud=0.1,\n",
        "    fliplr=0.5,\n",
        "    mosaic=1.0,\n",
        "    label_smoothing=0.01,\n",
        "    warmup_epochs=5,\n",
        "    box=0.05,\n",
        "    cls=0.3,\n",
        "    dfl=1.5,\n",
        "    project='tumor_detection_runs',\n",
        "    name='custom_detector',\n",
        "    seed=42,\n",
        "    device='cuda'\n",
        ")\n",
        "\n",
        "#  Evaluation\n",
        "best_model = YOLO(\"tumor_detection_runs/custom_detector/weights/best.pt\")\n",
        "metrics = best_model.val()\n",
        "\n",
        "# Save results\n",
        "f1 = 2 * (metrics.box.mp * metrics.box.mr) / (metrics.box.mp + metrics.box.mr + 1e-6)\n",
        "results = {\n",
        "    \"Precision\": metrics.box.mp,\n",
        "    \"Recall\": metrics.box.mr,\n",
        "    \"F1 Score\": f1,\n",
        "    \"mAP@0.5\": metrics.box.map50,\n",
        "    \"mAP@0.5:0.95\": metrics.box.map\n",
        "}\n",
        "df = pd.DataFrame([results])\n",
        "df.to_csv(\"bbox_eval_metrics.csv\", index=False)\n",
        "\n",
        "#  Display Metrics Summary\n",
        "print(\"\\n Evaluation Metrics:\")\n",
        "print(f\" - Precision      : {metrics.box.mp:.4f}\")\n",
        "print(f\" - Recall         : {metrics.box.mr:.4f}\")\n",
        "print(f\" - F1 Score       : {f1:.4f}\")\n",
        "print(f\" - mAP@0.5        : {metrics.box.map50:.4f}\")\n",
        "print(f\" - mAP@0.5:0.95   : {metrics.box.map:.4f}\")\n",
        "\n",
        "# Treat F1 as pseudo-accuracy for interpretation\n",
        "print(f\"\\n Pseudo-Accuracy (F1-Score as proxy): {f1:.4f}\")\n",
        "\n",
        "# Predict\n",
        "best_model.predict(source=base / \"images/val\", save=True, conf=0.25)\n",
        "print(\" Inference results saved to: runs/detect/predict\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65P9sahjeiNE",
        "outputId": "53cf19b2-b2c2-4568-9cec-715e9906231f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.5M/21.5M [00:00<00:00, 178MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ 'label_smoothing' is deprecated and will be removed in in the future.\n",
            "Ultralytics 8.3.105 ğŸš€ Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=yolo_dataset/tumor_data.yaml, epochs=150, time=None, patience=20, batch=16, imgsz=512, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=tumor_detection_runs, name=custom_detector, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.2, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.002, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=5, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.4, shear=2.0, perspective=0.0005, flipud=0.1, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.1, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=tumor_detection_runs/custom_detector\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 14.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir tumor_detection_runs/custom_detector', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 64.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolo_dataset/labels/train... 368 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:00<00:00, 1569.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolo_dataset/labels/train.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/labels/val... 91 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:00<00:00, 1397.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolo_dataset/labels/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to tumor_detection_runs/custom_detector/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 512 train, 512 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mtumor_detection_runs/custom_detector\u001b[0m\n",
            "Starting training for 150 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      1/150      2.38G    0.01225      5.226      1.447         27        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:03<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.332     0.0323     0.0292     0.0198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      2/150      2.93G   0.009081     0.7613      1.198         36        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.54it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.795      0.753      0.818      0.506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      3/150      2.96G   0.008911     0.7114        1.2         22        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.629      0.796      0.749      0.467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      4/150         3G   0.008732     0.6498      1.194         30        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93     0.0514      0.204     0.0503     0.0177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      5/150      3.04G   0.008333     0.6443       1.19         32        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.845      0.903      0.882      0.577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      6/150      3.07G   0.008936     0.6735       1.22         24        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.659      0.516      0.619      0.353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      7/150      3.11G   0.008256     0.5622      1.165         31        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 12.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.786      0.774      0.858      0.511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      8/150      3.14G   0.008561     0.5557      1.182         25        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.975      0.849      0.919      0.602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      9/150      3.18G   0.008775      0.557      1.208         25        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.829      0.871      0.908      0.608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     10/150      3.21G   0.008366     0.5542       1.18         19        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.98it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.938      0.817      0.871        0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     11/150      3.25G   0.008762     0.5303      1.196         44        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.763      0.829      0.791      0.501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     12/150      3.29G   0.008336     0.5278      1.178         22        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 10.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93       0.91      0.869      0.907      0.613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     13/150      3.32G   0.008286     0.4941      1.178         26        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.894      0.818      0.875      0.618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     14/150      3.57G   0.008004     0.5082       1.17         27        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.895      0.826      0.902      0.612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     15/150      3.61G   0.008464     0.5336      1.195         27        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 12.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.852      0.866      0.897      0.596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     16/150      3.65G   0.008082     0.5072      1.165         26        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.845      0.878      0.913      0.603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     17/150      3.68G   0.008163     0.4972      1.199         30        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 12.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.895      0.849      0.928      0.625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     18/150      3.72G   0.007918      0.465       1.17         24        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.856      0.871      0.915      0.641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     19/150      3.75G   0.007777     0.4825      1.147         27        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.946      0.828       0.91      0.616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     20/150      3.79G   0.007368     0.4418      1.112         27        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 10.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.942      0.903      0.939      0.629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     21/150      3.83G     0.0078     0.4713      1.132         28        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.909      0.862      0.903      0.617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     22/150      3.86G   0.008042     0.4609       1.14         37        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.831      0.871      0.905      0.597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     23/150       3.9G   0.008098     0.4511      1.142         31        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.942      0.849      0.918      0.659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     24/150      3.94G   0.007963     0.4568      1.161         20        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.927      0.871      0.924      0.628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     25/150      3.97G   0.007395     0.4131      1.107         31        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.943      0.894      0.934      0.678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     26/150      4.22G   0.007532     0.4349      1.104         31        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.942      0.892      0.925      0.679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     27/150      4.26G   0.007444     0.4341      1.129         22        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.911      0.886      0.924       0.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     28/150      4.29G   0.007579     0.4291      1.137         34        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.912      0.882      0.924      0.632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     29/150      4.33G   0.007732     0.4445      1.134         34        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93       0.92      0.903      0.938      0.637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     30/150      4.37G   0.007463     0.4037      1.135         30        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.871      0.867       0.91       0.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     31/150       4.4G   0.007483     0.4167      1.097         38        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.916      0.892      0.922      0.663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     32/150      4.44G    0.00724     0.4419      1.107         24        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.927      0.839       0.91      0.647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     33/150      4.48G   0.007657     0.4656      1.143         19        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.918      0.845      0.924      0.625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     34/150      4.51G     0.0076     0.4307      1.129         23        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.939      0.882      0.944      0.658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     35/150      4.55G   0.007115     0.3916      1.085         27        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.71it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.941      0.892      0.946      0.697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     36/150      4.94G   0.007225     0.3938      1.087         25        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 12.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93       0.91      0.925      0.928      0.685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     37/150      4.98G   0.007114     0.3759      1.099         28        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.964      0.872      0.941      0.662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     38/150      5.01G   0.007246     0.3923      1.091         25        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.954      0.893      0.937      0.651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     39/150      5.05G   0.007518     0.4174      1.119         26        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.905      0.849      0.922       0.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     40/150      5.09G   0.007503     0.4205      1.105         25        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 12.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93       0.92      0.865      0.918      0.633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     41/150      5.12G    0.00737     0.3902      1.133         20        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.924      0.917      0.936      0.645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     42/150      5.16G   0.007179     0.3798      1.096         30        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.942      0.892       0.93      0.681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     43/150       5.2G   0.007384     0.3972      1.112         26        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.932      0.881      0.935       0.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     44/150      5.23G    0.00729     0.3895      1.123         36        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 12.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.909      0.914      0.934      0.634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     45/150      5.27G   0.006968     0.3842      1.118         28        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.975      0.892      0.944      0.681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     46/150       5.3G   0.007086     0.3881      1.092         29        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.933      0.903      0.938      0.681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     47/150      5.34G    0.00728      0.396      1.093         30        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 12.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.968      0.882      0.946      0.681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     48/150      5.59G   0.007161     0.3992      1.095         33        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.926      0.941      0.953      0.671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     49/150      5.63G   0.006874     0.3723      1.082         32        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.977      0.902      0.953      0.676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     50/150      5.66G    0.00729     0.3878      1.108         26        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.935      0.934      0.955      0.648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     51/150       5.7G   0.007172     0.3924      1.112         27        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 12.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.904      0.913      0.937      0.662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     52/150      5.74G   0.007155     0.3735      1.129         26        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93       0.89      0.871      0.917      0.632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     53/150      5.77G   0.006898     0.3798      1.083         30        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.944      0.907      0.931       0.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     54/150      5.81G   0.007137     0.3724      1.113         23        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.944      0.908      0.932      0.675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     55/150      5.84G   0.007294     0.3909      1.109         33        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 11.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.956      0.931      0.947       0.67\n",
            "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 20 epochs. Best results observed at epoch 35, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=20) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "55 epochs completed in 0.044 hours.\n",
            "Optimizer stripped from tumor_detection_runs/custom_detector/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from tumor_detection_runs/custom_detector/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating tumor_detection_runs/custom_detector/weights/best.pt...\n",
            "Ultralytics 8.3.105 ğŸš€ Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.941      0.892      0.946      0.697\n",
            "Speed: 0.1ms preprocess, 0.6ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
            "Results saved to \u001b[1mtumor_detection_runs/custom_detector\u001b[0m\n",
            "Ultralytics 8.3.105 ğŸš€ Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/labels/val.cache... 91 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         91         93      0.941      0.892      0.945      0.695\n",
            "Speed: 1.5ms preprocess, 2.9ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "\n",
            " Evaluation Metrics:\n",
            " - Precision      : 0.9407\n",
            " - Recall         : 0.8925\n",
            " - F1 Score       : 0.9159\n",
            " - mAP@0.5        : 0.9449\n",
            " - mAP@0.5:0.95   : 0.6953\n",
            "\n",
            " Pseudo-Accuracy (F1-Score as proxy): 0.9159\n",
            "\n",
            "image 1/91 /content/yolo_dataset/images/val/00056_239.jpg: 512x512 1 tumor, 8.0ms\n",
            "image 2/91 /content/yolo_dataset/images/val/00058_179.jpg: 512x512 (no detections), 7.8ms\n",
            "image 3/91 /content/yolo_dataset/images/val/00063_197.jpg: 512x512 1 tumor, 7.5ms\n",
            "image 4/91 /content/yolo_dataset/images/val/00066_240.jpg: 512x512 1 tumor, 7.6ms\n",
            "image 5/91 /content/yolo_dataset/images/val/00066_259.jpg: 512x512 1 tumor, 7.4ms\n",
            "image 6/91 /content/yolo_dataset/images/val/00071_183.jpg: 512x512 1 tumor, 7.6ms\n",
            "image 7/91 /content/yolo_dataset/images/val/00074_229.jpg: 512x512 1 tumor, 7.4ms\n",
            "image 8/91 /content/yolo_dataset/images/val/00077_186.jpg: 512x512 1 tumor, 7.6ms\n",
            "image 9/91 /content/yolo_dataset/images/val/00078_239.jpg: 512x512 1 tumor, 7.5ms\n",
            "image 10/91 /content/yolo_dataset/images/val/00096_240.jpg: 512x512 1 tumor, 7.6ms\n",
            "image 11/91 /content/yolo_dataset/images/val/00098_247.jpg: 512x512 1 tumor, 8.0ms\n",
            "image 12/91 /content/yolo_dataset/images/val/00100_153.jpg: 512x512 1 tumor, 7.6ms\n",
            "image 13/91 /content/yolo_dataset/images/val/00100_170.jpg: 512x512 1 tumor, 7.5ms\n",
            "image 14/91 /content/yolo_dataset/images/val/00136_103.jpg: 512x512 1 tumor, 7.8ms\n",
            "image 15/91 /content/yolo_dataset/images/val/00136_121.jpg: 512x512 1 tumor, 7.6ms\n",
            "image 16/91 /content/yolo_dataset/images/val/00138_217.jpg: 512x512 (no detections), 7.5ms\n",
            "image 17/91 /content/yolo_dataset/images/val/00138_289.jpg: 512x512 1 tumor, 7.4ms\n",
            "image 18/91 /content/yolo_dataset/images/val/00144_243.jpg: 512x512 1 tumor, 8.7ms\n",
            "image 19/91 /content/yolo_dataset/images/val/00146_160.jpg: 512x512 1 tumor, 7.6ms\n",
            "image 20/91 /content/yolo_dataset/images/val/00156_164.jpg: 512x512 1 tumor, 7.3ms\n",
            "image 21/91 /content/yolo_dataset/images/val/00159_133.jpg: 512x512 1 tumor, 7.4ms\n",
            "image 22/91 /content/yolo_dataset/images/val/00160_270.jpg: 512x512 1 tumor, 7.5ms\n",
            "image 23/91 /content/yolo_dataset/images/val/00171_126.jpg: 512x512 1 tumor, 7.7ms\n",
            "image 24/91 /content/yolo_dataset/images/val/00177_221.jpg: 512x512 1 tumor, 7.5ms\n",
            "image 25/91 /content/yolo_dataset/images/val/00178_104.jpg: 512x512 1 tumor, 7.8ms\n",
            "image 26/91 /content/yolo_dataset/images/val/00178_140.jpg: 512x512 1 tumor, 7.8ms\n",
            "image 27/91 /content/yolo_dataset/images/val/00185_271.jpg: 512x512 1 tumor, 9.0ms\n",
            "image 28/91 /content/yolo_dataset/images/val/00185_289.jpg: 512x512 1 tumor, 9.3ms\n",
            "image 29/91 /content/yolo_dataset/images/val/00187_100.jpg: 512x512 1 tumor, 9.1ms\n",
            "image 30/91 /content/yolo_dataset/images/val/00187_118.jpg: 512x512 1 tumor, 7.3ms\n",
            "image 31/91 /content/yolo_dataset/images/val/00240_144.jpg: 512x384 1 tumor, 61.0ms\n",
            "image 32/91 /content/yolo_dataset/images/val/00250_115.jpg: 512x384 1 tumor, 8.2ms\n",
            "image 33/91 /content/yolo_dataset/images/val/00250_121.jpg: 512x384 1 tumor, 8.1ms\n",
            "image 34/91 /content/yolo_dataset/images/val/00260_121.jpg: 512x384 1 tumor, 7.9ms\n",
            "image 35/91 /content/yolo_dataset/images/val/00263_123.jpg: 512x384 1 tumor, 7.7ms\n",
            "image 36/91 /content/yolo_dataset/images/val/00263_125.jpg: 512x384 1 tumor, 8.0ms\n",
            "image 37/91 /content/yolo_dataset/images/val/00270_12.jpg: 512x448 1 tumor, 61.3ms\n",
            "image 38/91 /content/yolo_dataset/images/val/00271_116.jpg: 512x384 1 tumor, 8.9ms\n",
            "image 39/91 /content/yolo_dataset/images/val/00271_145.jpg: 512x384 1 tumor, 7.7ms\n",
            "image 40/91 /content/yolo_dataset/images/val/00273_86.jpg: 512x384 1 tumor, 7.5ms\n",
            "image 41/91 /content/yolo_dataset/images/val/00281_110.jpg: 512x512 1 tumor, 8.0ms\n",
            "image 42/91 /content/yolo_dataset/images/val/00282_83.jpg: 512x384 1 tumor, 10.0ms\n",
            "image 43/91 /content/yolo_dataset/images/val/00282_87.jpg: 512x384 1 tumor, 8.0ms\n",
            "image 44/91 /content/yolo_dataset/images/val/00282_93.jpg: 512x384 1 tumor, 8.4ms\n",
            "image 45/91 /content/yolo_dataset/images/val/00305_113.jpg: 512x512 1 tumor, 8.3ms\n",
            "image 46/91 /content/yolo_dataset/images/val/00305_116.jpg: 512x512 1 tumor, 7.6ms\n",
            "image 47/91 /content/yolo_dataset/images/val/00306_102.jpg: 512x384 1 tumor, 8.4ms\n",
            "image 48/91 /content/yolo_dataset/images/val/00306_109.jpg: 512x384 1 tumor, 8.2ms\n",
            "image 49/91 /content/yolo_dataset/images/val/00311_105.jpg: 512x384 1 tumor, 8.0ms\n",
            "image 50/91 /content/yolo_dataset/images/val/00313_126.jpg: 512x384 1 tumor, 7.9ms\n",
            "image 51/91 /content/yolo_dataset/images/val/00317_24.jpg: 512x512 1 tumor, 8.0ms\n",
            "image 52/91 /content/yolo_dataset/images/val/00322_130.jpg: 512x384 2 tumors, 8.1ms\n",
            "image 53/91 /content/yolo_dataset/images/val/00322_134.jpg: 512x384 2 tumors, 8.0ms\n",
            "image 54/91 /content/yolo_dataset/images/val/00332_141.jpg: 512x384 (no detections), 7.9ms\n",
            "image 55/91 /content/yolo_dataset/images/val/00334_108.jpg: 512x384 1 tumor, 7.8ms\n",
            "image 56/91 /content/yolo_dataset/images/val/00334_121.jpg: 512x384 1 tumor, 7.8ms\n",
            "image 57/91 /content/yolo_dataset/images/val/00334_95.jpg: 512x384 1 tumor, 7.9ms\n",
            "image 58/91 /content/yolo_dataset/images/val/00340_121.jpg: 512x384 1 tumor, 7.8ms\n",
            "image 59/91 /content/yolo_dataset/images/val/00350_128.jpg: 512x384 1 tumor, 7.9ms\n",
            "image 60/91 /content/yolo_dataset/images/val/00350_139.jpg: 512x384 1 tumor, 8.0ms\n",
            "image 61/91 /content/yolo_dataset/images/val/109 (2).jpg: 512x512 1 tumor, 8.0ms\n",
            "image 62/91 /content/yolo_dataset/images/val/62 (12).jpg: 512x512 1 tumor, 7.5ms\n",
            "image 63/91 /content/yolo_dataset/images/val/62 (13).jpg: 512x512 1 tumor, 7.7ms\n",
            "image 64/91 /content/yolo_dataset/images/val/65 (8).jpg: 512x512 2 tumors, 7.4ms\n",
            "image 65/91 /content/yolo_dataset/images/val/66 (13).jpg: 512x512 (no detections), 7.4ms\n",
            "image 66/91 /content/yolo_dataset/images/val/66 (7).jpg: 512x512 1 tumor, 8.1ms\n",
            "image 67/91 /content/yolo_dataset/images/val/66 (9).jpg: 512x512 1 tumor, 7.8ms\n",
            "image 68/91 /content/yolo_dataset/images/val/68 (4).jpg: 512x512 1 tumor, 7.5ms\n",
            "image 69/91 /content/yolo_dataset/images/val/68 (5).jpg: 512x512 1 tumor, 7.4ms\n",
            "image 70/91 /content/yolo_dataset/images/val/68 (9).jpg: 512x512 1 tumor, 7.4ms\n",
            "image 71/91 /content/yolo_dataset/images/val/70 (7).jpg: 512x512 1 tumor, 7.9ms\n",
            "image 72/91 /content/yolo_dataset/images/val/71 (4).jpg: 512x512 1 tumor, 7.5ms\n",
            "image 73/91 /content/yolo_dataset/images/val/72 (11).jpg: 512x512 1 tumor, 7.6ms\n",
            "image 74/91 /content/yolo_dataset/images/val/72 (7).jpg: 512x512 (no detections), 7.3ms\n",
            "image 75/91 /content/yolo_dataset/images/val/74 (11).jpg: 512x512 1 tumor, 7.3ms\n",
            "image 76/91 /content/yolo_dataset/images/val/74 (12).jpg: 512x512 1 tumor, 7.9ms\n",
            "image 77/91 /content/yolo_dataset/images/val/75 (5).jpg: 512x512 1 tumor, 7.8ms\n",
            "image 78/91 /content/yolo_dataset/images/val/76 (7).jpg: 512x512 1 tumor, 8.1ms\n",
            "image 79/91 /content/yolo_dataset/images/val/79 (10).jpg: 512x512 1 tumor, 7.7ms\n",
            "image 80/91 /content/yolo_dataset/images/val/79 (12).jpg: 512x512 1 tumor, 7.6ms\n",
            "image 81/91 /content/yolo_dataset/images/val/79 (9).jpg: 512x512 1 tumor, 7.6ms\n",
            "image 82/91 /content/yolo_dataset/images/val/83 (3).jpg: 512x512 1 tumor, 7.8ms\n",
            "image 83/91 /content/yolo_dataset/images/val/84 (3).jpg: 512x512 1 tumor, 7.9ms\n",
            "image 84/91 /content/yolo_dataset/images/val/84 (4).jpg: 512x512 1 tumor, 7.9ms\n",
            "image 85/91 /content/yolo_dataset/images/val/87 (5).jpg: 512x512 1 tumor, 8.5ms\n",
            "image 86/91 /content/yolo_dataset/images/val/89 (5).jpg: 512x512 (no detections), 8.0ms\n",
            "image 87/91 /content/yolo_dataset/images/val/91 (6).jpg: 512x512 1 tumor, 7.7ms\n",
            "image 88/91 /content/yolo_dataset/images/val/94 (5).jpg: 512x512 1 tumor, 7.8ms\n",
            "image 89/91 /content/yolo_dataset/images/val/96 (4).jpg: 512x512 1 tumor, 7.7ms\n",
            "image 90/91 /content/yolo_dataset/images/val/97 (2).jpg: 512x512 1 tumor, 7.7ms\n",
            "image 91/91 /content/yolo_dataset/images/val/98 (4).jpg: 512x512 1 tumor, 7.7ms\n",
            "Speed: 1.5ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 512)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
            " Inference results saved to: runs/detect/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configs\n",
        "IMG_SIZE = 512\n",
        "THRESHOLD = 0.5\n",
        "TEST_DIR = \"PATH\"\n",
        "CLASSIFIER_PATH = \"/content/optimized_classifier.keras\"\n",
        "DETECTOR_PATH = \"/content/yolov8s.pt\"\n",
        "\n",
        "#  Load models\n",
        "clf_model = load_model(CLASSIFIER_PATH)\n",
        "yolo_model = YOLO(DETECTOR_PATH)\n",
        "\n",
        "#  Image preprocessing\n",
        "def preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        return None\n",
        "    h, w = img.shape\n",
        "    scale = IMG_SIZE / max(h, w)\n",
        "    new_w, new_h = int(w * scale), int(h * scale)\n",
        "    resized = cv2.resize(img, (new_w, new_h))\n",
        "    top = (IMG_SIZE - new_h) // 2\n",
        "    bottom = IMG_SIZE - new_h - top\n",
        "    left = (IMG_SIZE - new_w) // 2\n",
        "    right = IMG_SIZE - new_w - left\n",
        "    padded = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
        "    rgb = np.repeat(padded[..., np.newaxis], 3, axis=-1)\n",
        "    processed = preprocess_input(rgb.astype('float32'))\n",
        "    return np.expand_dims(processed, axis=0)\n",
        "\n",
        "#  Run Pipeline\n",
        "\n",
        "image_paths = sorted([os.path.join(TEST_DIR, f) for f in os.listdir(TEST_DIR) if f.lower().endswith(('.jpg', '.png'))])\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "print(f\"\\n Processing {len(image_paths)} test images...\\n\")\n",
        "\n",
        "for path in tqdm(image_paths):\n",
        "    img_name = os.path.basename(path)\n",
        "    img_tensor = preprocess_image(path)\n",
        "\n",
        "    if img_tensor is None:\n",
        "        print(f\" Skipped: {img_name} (could not load)\")\n",
        "        continue\n",
        "\n",
        "    # Run classification\n",
        "    prob = clf_model.predict(img_tensor, verbose=0)[0][0]\n",
        "    label = int(prob >= THRESHOLD)\n",
        "    y_pred.append(label)\n",
        "\n",
        "    # Inferred true label from filename\n",
        "    gt = 1 if \"tumor\" in img_name.lower() else 0\n",
        "    y_true.append(gt)\n",
        "\n",
        "    print(f\" {img_name} | Tumor Probability: {prob:.4f} â†’ {'Tumor' if label else 'No Tumor'}\")\n",
        "\n",
        "    # YOLO detection if tumor predicted\n",
        "    if label:\n",
        "        yolo_model.predict(source=path, save=True, conf=0.25)\n",
        "        print(\" Detection saved to: runs/detect/predict\")\n",
        "\n",
        "#  Results\n",
        "print(\"\\n Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\n Overall Classification Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "wfM06aDDnurz",
        "outputId": "a0f0229c-6331-43c0-8d7a-b633135427f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimage_paths = sorted([os.path.join(TEST_DIR, f) for f in os.listdir(TEST_DIR) if f.lower().endswith((\\'.jpg\\', \\'.png\\'))])\\ny_true, y_pred = [], []\\n\\nprint(f\"\\n Processing {len(image_paths)} test images...\\n\")\\n\\nfor path in tqdm(image_paths):\\n    img_name = os.path.basename(path)\\n    img_tensor = preprocess_image(path)\\n\\n    if img_tensor is None:\\n        print(f\" Skipped: {img_name} (could not load)\")\\n        continue\\n\\n    # Run classification\\n    prob = clf_model.predict(img_tensor, verbose=0)[0][0]\\n    label = int(prob >= THRESHOLD)\\n    y_pred.append(label)\\n\\n    # Inferred true label from filename\\n    gt = 1 if \"tumor\" in img_name.lower() else 0\\n    y_true.append(gt)\\n\\n    print(f\" {img_name} | Tumor Probability: {prob:.4f} â†’ {\\'Tumor\\' if label else \\'No Tumor\\'}\")\\n\\n    # YOLO detection if tumor predicted\\n    if label:\\n        yolo_model.predict(source=path, save=True, conf=0.25)\\n        print(\" Detection saved to: runs/detect/predict\")\\n\\n#  Results \\nprint(\"\\n Classification Report:\")\\nprint(classification_report(y_true, y_pred, digits=4))\\n\\nacc = accuracy_score(y_true, y_pred)\\nprint(f\"\\n Overall Classification Accuracy: {acc:.4f}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}